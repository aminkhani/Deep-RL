---
jupyter:
  accelerator: GPU
  colab:
    name: space_invader_dqn.ipynb
  gpuClass: standard
  kernelspec:
    display_name: Python 3 (ipykernel)
    language: python
    name: python3
  language_info:
    codemirror_mode:
      name: ipython
      version: 3
    file_extension: .py
    mimetype: text/x-python
    name: python
    nbconvert_exporter: python
    pygments_lexer: ipython3
    version: 3.10.4
  nbformat: 4
  nbformat_minor: 1
  vscode:
    interpreter:
      hash: aafc33a19d1e6c143b5a4b283b91d6d8d620b2aadbd9675105175468fd18e65b
---

::: {.cell .markdown id="WpABiUpzhVOZ"}
## Space Invaders With DQN
:::

::: {.cell .markdown id="zIyhgtTrhfzh"}
## Step 0

1- For using Space Invaders in Colab we need to download ROMS and Import
the environment

2- To displaye the Agent & Environment we use env.render() but not work
in colab, for runing env.render() use the below cells
:::

::: {.cell .code id="zdAcCZ4RZik9"}
``` {.python}
# 1- Run this cell to Import the environment
! wget http://www.atarimania.com/roms/Roms.rar
! mkdir /content/ROM/
! unrar e /content/Roms.rar /content/ROM/
! python -m atari_py.import_roms /content/ROM/
```
:::

::: {.cell .code id="kNioA4kYfD28"}
``` {.python}
# 2- Dowload and install requirements
!pip install gym pyvirtualdisplay > /dev/null 2>&1
!apt-get install -y xvfb python-opengl ffmpeg > /dev/null 2>&1
!apt-get update > /dev/null 2>&1
!apt-get install cmake > /dev/null 2>&1
!pip install --upgrade setuptools 2>&1
!pip install ez_setup > /dev/null 2>&1
!pip install gym[atari] > /dev/null 2>&1
```
:::

::: {.cell .code execution_count="1" id="3UErC4-9fH27"}
``` {.python}
# Next, we define the functions used to show the video by adding it to the CoLab 
import gym
from gym.wrappers import Monitor
import glob
import io
import base64
from IPython.display import HTML
from pyvirtualdisplay import Display
from IPython import display as ipythondisplay

display = Display(visible=0, size=(1400, 900))
display.start()

"""
Utility functions to enable video recording of gym environment 
and displaying it.
To enable video, just do "env = wrap_env(env)""
"""


def show_video():
    mp4list = glob.glob('video/*.mp4')
    if len(mp4list) > 0:
        mp4 = mp4list[0]
        video = io.open(mp4, 'r+b').read()
        encoded = base64.b64encode(video)
        ipythondisplay.display(HTML(data='''<video alt="test" autoplay 
                loop controls style="height: 400px;">
                <source src="data:video/mp4;base64,{0}" type="video/mp4" />
             </video>'''.format(encoded.decode('ascii'))))
    else:
        print("Could not find video")


def wrap_env(env):
    env = Monitor(env, './video', force=True)
    return env
```
:::

::: {.cell .markdown id="LN0nZwyMGadB"}
## Step 1: Import the libraries
:::

::: {.cell .code execution_count="2" id="RF19XeI0V4DN"}
``` {.python}
import time
import gym
from gym import logger as gymlogger
from gym.wrappers import Monitor
gymlogger.set_level(30)
import random
import torch
import numpy as np
from collections import deque
import matplotlib.pyplot as plt
import math
```
:::

::: {.cell .code execution_count="3" id="pg4m4pysV4DQ"}
``` {.python}
# Local Libraries
from dqn_agent import DQNAgent
from dqn_cnn import DQNCnn
from stack_frame import preprocess_frame, stack_frame
```
:::

::: {.cell .markdown id="tfo8jleHGadK"}
## Step 2: Create our environment

Initialize the environment in the code cell below.
:::

::: {.cell .code execution_count="4" colab="{\"base_uri\":\"https://localhost:8080/\"}" id="t4NLWW8HV4DR" outputId="80231f09-3071-446d-a37c-93563543416b"}
``` {.python}
env = wrap_env(gym.make('SpaceInvaders-v0'))
env.seed(0)
```

::: {.output .execute_result execution_count="4"}
    [0, 592379725]
:::
:::

::: {.cell .code execution_count="5" colab="{\"base_uri\":\"https://localhost:8080/\"}" id="lnYIMwuEV4DS" outputId="434ac1e1-0c4f-4af5-9b62-21803b02dec7"}
``` {.python}
# if gpu is to be used
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print("Device: ", device)
```

::: {.output .stream .stdout}
    Device:  cuda
:::
:::

::: {.cell .markdown id="nS221MgXGadP"}
## Step 3: Viewing our Enviroment
:::

::: {.cell .code execution_count="6" colab="{\"height\":350,\"base_uri\":\"https://localhost:8080/\"}" id="u7InfF_kV4DU" outputId="2ade6f06-05d1-4e7e-d0b4-1c7f57eb5576"}
``` {.python}
print("The size of frame is: ", env.observation_space.shape)
print("No. of Actions: ", env.action_space.n)
print("\n")
env.reset()
plt.figure()
plt.imshow(env.reset())
plt.title('Original Frame')
plt.show()
```

::: {.output .stream .stdout}
    The size of frame is:  (210, 160, 3)
    No. of Actions:  6
:::

::: {.output .display_data}
![](vertopal_dab6f7876e4d49c99c5bd13b717a683c/ffdb27c9be8358fd85fb1b3d58d4d3a05e041612.png)
:::
:::

::: {.cell .markdown id="dQ7JEkuTV4DV"}
### Execute the code cell below to play with a random policy. {#execute-the-code-cell-below-to-play-with-a-random-policy}
:::

::: {.cell .code execution_count="7" colab="{\"height\":439,\"base_uri\":\"https://localhost:8080/\"}" id="j0Mv4hxBV4DV" outputId="7c16bf91-cb54-4066-b8f6-1933671e6a2b"}
``` {.python}
def random_play():
    score = 0
    env.reset()
    while True:
        env.render()
        action = env.action_space.sample()
        state, reward, done, _ = env.step(action)
        score += reward
        if done:
            env.close()
            show_video()
            print("Your Score at end of game is: ", score)
            break
random_play()
```

::: {.output .display_data}
```{=html}
<video alt="test" autoplay 
                loop controls style="height: 400px;">
                <source src="data:video/mp4;base64,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" type="video/mp4" />
             </video>
```
:::

::: {.output .stream .stdout}
    Your Score at end of game is:  80.0
:::
:::

::: {.cell .markdown id="Sr52nmcpGada"}
# Step 4: Preprocessing Frame
:::

::: {.cell .code id="Tm0lcWXfV4DW"}
``` {.python}
env.reset()
plt.figure()
plt.imshow(preprocess_frame(env.reset(), (8, -12, -12, 4), 84), cmap="gray")
plt.title('Pre Processed image')
plt.show()
```
:::

::: {.cell .markdown id="mJMc3HA8Gade"}
## Step 5: Stacking Frame
:::

::: {.cell .code id="mCS0FjePV4DX"}
``` {.python}
def stack_frames(frames, state, is_new=False):
    frame = preprocess_frame(state, (8, -12, -12, 4), 84)
    frames = stack_frame(frames, frame, is_new)

    return frames
```
:::

::: {.cell .markdown id="bH22GTfhV4DY"}
## Step 6: Creating our Agent
:::

::: {.cell .code id="kBIKZNalV4DY"}
``` {.python}
INPUT_SHAPE = (4, 84, 84)
ACTION_SIZE = env.action_space.n
SEED = 0
GAMMA = 0.99           # discount factor
BUFFER_SIZE = 100000   # replay buffer size
BATCH_SIZE = 64        # Update batch size
LR = 0.0001            # learning rate 
TAU = 1e-3             # for soft update of target parameters
UPDATE_EVERY = 1       # how often to update the network
UPDATE_TARGET = 10000  # After which thershold replay to be started 
EPS_START = 0.99       # starting value of epsilon
EPS_END = 0.01         # Ending value of epsilon
EPS_DECAY = 100         # Rate by which epsilon to be decayed

agent = DQNAgent(INPUT_SHAPE, ACTION_SIZE, SEED, device, BUFFER_SIZE, BATCH_SIZE, GAMMA, LR, TAU, UPDATE_EVERY, UPDATE_TARGET, DQNCnn)
```
:::

::: {.cell .markdown id="reH8jzUTV4DZ"}
## Step 7: Watching untrained agent play
:::

::: {.cell .code id="XEe1jAfBV4DZ"}
``` {.python}
# watch an untrained agent
state = stack_frames(None, env.reset(), True) 
for j in range(200):
    env.render()
    action = agent.act(state)
    next_state, reward, done, _ = env.step(action)
    state = stack_frames(state, next_state, False)
    if done:
        break 
        
env.close()
show_video()
```
:::

::: {.cell .markdown id="Fl6HKp9GV4DZ"}
## Step 8: Loading Agent

Uncomment line to load a pretrained agent
:::

::: {.cell .code id="WLTlfl00V4DZ"}
``` {.python}
start_epoch = 0
scores = []
scores_window = deque(maxlen=20)
```
:::

::: {.cell .markdown id="MYqtjUgQV4Da"}
## Step 9: Train the Agent with DQN
:::

::: {.cell .code id="WOt-aLHsV4Da"}
``` {.python}
epsilon_by_epsiode = lambda frame_idx: EPS_END + (EPS_START - EPS_END) * math.exp(-1. * frame_idx /EPS_DECAY)

plt.plot([epsilon_by_epsiode(i) for i in range(1000)])
```
:::

::: {.cell .code id="3outDx_1V4Da"}
``` {.python}
def train(n_episodes=1000):
    """
    Params
    ======
        n_episodes (int): maximum number of training episodes
    """
    for i_episode in range(start_epoch + 1, n_episodes+1):
        state = stack_frames(None, env.reset(), True)
        score = 0
        eps = epsilon_by_epsiode(i_episode)
        while True:
            action = agent.act(state, eps)
            next_state, reward, done, info = env.step(action)
            score += reward
            next_state = stack_frames(state, next_state, False)
            agent.step(state, action, reward, next_state, done)
            state = next_state
            if done:
                break
        scores_window.append(score)       # save most recent score
        scores.append(score)              # save most recent score
        print('\rEpisode {}\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)), end="")
        
        if i_episode % 100 == 0:
            print('\rEpisode {}\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)))
            fig = plt.figure()
            ax = fig.add_subplot(111)
            plt.plot(np.arange(len(scores)), scores)
            plt.ylabel('Score')
            plt.xlabel('Episode #')
            plt.show()
    
    return scores
```
:::

::: {.cell .code id="tDuOnPGVV4Db"}
``` {.python}
scores = train(1000)
```
:::

::: {.cell .markdown id="jANzNpp-V4Db"}
## Step 10: Watch a Smart Agent!
:::

::: {.cell .code id="Fs7kQ5S2V4Dc"}
``` {.python}
score = 0
state = stack_frames(None, env.reset(), True)
while True:
    env.render()
    action = agent.act(state)
    next_state, reward, done, _ = env.step(action)
    score += reward
    state = stack_frames(state, next_state, False)
    if done:
        print("You Final score is:", score)
        break 
env.close()
show_video()
```
:::
